{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/06_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 6: Neural Networks\n",
    "\n",
    "### 6.1. Learning a neural net with scikit-learn for the Adult dataset\n",
    "\n",
    "In this task, you should create a neural network for the Adult dataset. Use the MLPClassifier provided by scikit-learn.\n",
    "\n",
    "#### 6.1.1\tLoad the Adults dataset and play around with the hidden layers and the activation function (both are architecture parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "adult_arff_data, adult_arff_meta = arff.loadarff(open('adult.arff', 'r'))\n",
    "adult = pd.DataFrame(adult_arff_data)\n",
    "adult = adult.applymap(lambda x: x.decode('utf8').replace(\"'\", \"\") if hasattr(x, 'decode') else x)\n",
    "\n",
    "adult_target = adult['class']\n",
    "label_encoder = LabelEncoder()\n",
    "adult_target = label_encoder.fit_transform(adult_target)\n",
    "adult_data = adult.drop('class', axis=1)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for hidden_layers in [#TODO: define different hidden layers]:\n",
    "    print(\"=======\" + str(hidden_layers) + \"=======\")\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layers, activation='relu',\n",
    "                        learning_rate_init=1e-3, batch_size=128, verbose=True,\n",
    "                        early_stopping=True, random_state=1234)\n",
    "\n",
    "    pipeline = Pipeline([ ('preprocessing', preprocessor), ('estimator', clf) ])\n",
    "    pipeline.fit(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2\tPlay around with the hyperparameters of the neural network like learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for learning_rate in [#TODO: define different learning rates]:\n",
    "    print(\"=========== lr: \" + str(learning_rate) + \"===========\")\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
    "                        learning_rate_init=learning_rate, batch_size=128, verbose=True,\n",
    "                        early_stopping=True, random_state=1234)\n",
    "\n",
    "    pipeline = Pipeline([ ('preprocessing', preprocessor), ('estimator', clf) ])\n",
    "\n",
    "    pipeline.fit(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch_size in [#TODO: define different batch sizes]:\n",
    "    print(\"=========== batch_size: \" + str(batch_size) + \" ===========\")\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
    "                        learning_rate_init=1e-3, batch_size=batch_size, verbose=True,\n",
    "                        early_stopping=True, random_state=1234)\n",
    "\n",
    "    pipeline = Pipeline([ ('preprocessing', preprocessor), ('estimator', clf) ])\n",
    "\n",
    "    pipeline.fit(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Neural networks with PyTorch Lightning\n",
    "\n",
    "In the previous exercise only the MLPClassifier is used. To extend the possibilities, use PyTorch Lightning and create a similar neural network.\n",
    "\n",
    "#### 6.1.1\tImplement the same model in PyTorch Lightning and find an optimal model using F1 as the scoring measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# TODO: preprocess data, create TensorDataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_model = #TODO: define model\n",
    "        # TODO define metrics\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.my_model(x)\n",
    "        return logits\n",
    "\n",
    "    def make_step(self, x,y):\n",
    "        logits = self.forward(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.unsqueeze(-1))\n",
    "        predictions = torch.sigmoid(logits).squeeze()\n",
    "        return loss, predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, predictions = self.make_step(x,y)\n",
    "\n",
    "        #TODO: call metric\n",
    "        \n",
    "        return {'loss' : loss}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        avg_train_loss = torch.stack([x['loss'] for x in training_step_outputs]).mean()\n",
    "        \n",
    "        #TODO: compute epoch train f1 and print training loss \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, predictions = self.make_step(x,y)\n",
    "        \n",
    "        #TODO: call metric\n",
    "        return {'val_loss' : loss}\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in validation_step_outputs]).mean()\n",
    "        \n",
    "        #TODO: compute epoch validation f1 and print it\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, predictions = self.make_step(x,y)\n",
    "        #TODO: call metric\n",
    "        \n",
    "        return test_f1_batch\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        #TODO: log metric  \n",
    "        return avg_test_f1\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# set a seed for all libraries such as random, numpy random etc\n",
    "pl.seed_everything(42, workers=True)\n",
    "model = LightningModel()\n",
    "\n",
    "# TODO: use logged metric\n",
    "my_mectic = ''\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='./checkpoints', monitor=my_mectic)\n",
    "early_stop_callback = EarlyStopping(monitor=my_mectic, patience=10, mode=\"max\")\n",
    "trainer = pl.Trainer(deterministic=True, \n",
    "                     max_epochs=25,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback])\n",
    "\n",
    "trainer.fit(model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Multi class classification with PyTorch Lightning\n",
    "Use the Connect 4 dataset to adjust the neural network for multi class classification\n",
    "The target variable to predict is either ‘win’, ‘ loss’, or ‘ draw’.\n",
    "\n",
    "#### 6.3.1 Load the dataset and split it into training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : load dataset\n",
    "# TODO : make train validation test split\n",
    "# TODO : preprocess data, create TensorDataset and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Adapt the architecture (change the loss and adapt the output layer) and optimize for validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFourModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_model = # TODO define model\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.my_model(x)\n",
    "        return logits\n",
    "    \n",
    "    def make_step(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # TODO: calculate loss and predictions\n",
    "        # loss = \n",
    "        # predictions = \n",
    "        return logits, loss, predictions\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss, predictions = self.make_step(x,y)\n",
    "        self.train_acc(predictions, y)\n",
    "        return {'loss' : loss}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        avg_train_loss = torch.stack([x['loss'] for x in training_step_outputs]).mean()\n",
    "        avg_train_acc = self.train_acc.compute() \n",
    "        \n",
    "        self.log(\"loss/train\", avg_train_loss)\n",
    "        self.log(\"acc/train\", avg_train_acc)\n",
    "        \n",
    "        print('training loss at epoch ' + str(self.current_epoch) + ': ' + str(avg_train_loss.item()))\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss, predictions = self.make_step(x, y)\n",
    "        \n",
    "        self.valid_acc(predictions, y)\n",
    "        return {'val_loss' : loss} \n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in validation_step_outputs]).mean()\n",
    "        avg_val_acc = self.valid_acc.compute()\n",
    "        \n",
    "        self.log(\"loss/validation\", avg_val_loss)\n",
    "        self.log(\"acc/validation\", avg_val_acc)\n",
    "        \n",
    "        print('validation accuracy at epoch ' + str(self.current_epoch) + ': ' + str(avg_val_acc.item()))\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss, predictions = self.make_step(x, y)\n",
    "        test_acc_batch = self.test_acc(predictions, y)\n",
    "        return test_acc_batch\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_test_acc = self.test_acc.compute()        \n",
    "        self.log('acc/test', avg_test_acc)        \n",
    "        return avg_test_acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# set a seed for all libraries such as random, numpy random etc\n",
    "pl.seed_everything(42, workers=True)\n",
    "model = ConnectFourModel()\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='./checkpoints', monitor=\"acc/validation\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"acc/validation\", patience=10, mode=\"max\")\n",
    "\n",
    "trainer = pl.Trainer(deterministic=True, \n",
    "                     max_epochs=25,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback])\n",
    "\n",
    "trainer.fit(model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader, ckpt_path='best')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
