{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/07_TextMining'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 7: Text Mining\n",
    "\n",
    "### 7.1. Which documents are similar?\n",
    "\n",
    "#### 7.1.1. The file documents.zip is provided in ILIAS and contains three corpora. Load and vectorize the 4-documents corpus using the load_files function. How many different attributes has the generated example set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus_4_docs = load_files('DataSetEx7', categories=['corpus-4docs'], encoding='utf-8')\n",
    "\n",
    "# create a vectorizer and transform the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 947 attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.2.\tExamine the generated word list. What are the most common words? Look for the three most common words that might be helpful for text mining tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_word_list(X, Y, feature_names, target_names):\n",
    "    d = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "    doc = d[ d>0 ].count()\n",
    "    d = d.assign(target=Y)\n",
    "    d = d.groupby(by='target').sum()\n",
    "    d = d.transpose()\n",
    "    d.columns = target_names\n",
    "    total = d.sum(axis=1)\n",
    "    d = d.assign(total_occurrences=total)\n",
    "    d = d.assign(document_occurrences=doc)\n",
    "    d = d.sort_values(by='total_occurrences', ascending=False)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the word list from the transformed dataset and show it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Answer: Itâ€™s hard to find the most common word which would help to mine the text because the top words are so called stopwords. At position 30 you can find Madrid followed by United which may indicate a football game. At position 46 League is listed which underlines the first conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 7.1.3. Remove stopwords and apply the porter stemmer. By how many attributes do the operators reduce the size of your example set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\tobi1\\anaconda3\\envs\\dm1\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2021.10.23-cp36-cp36m-win_amd64.whl (273 kB)\n",
      "Requirement already satisfied: click in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from click->nltk) (4.8.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from importlib-metadata->click->nltk) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\tobi1\\anaconda3\\envs\\dm1\\lib\\site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.6.5 regex-2021.10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tobi1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re, string\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    stems = []\n",
    "    tokens = token_pattern.findall(text)\n",
    "    for item in tokens:\n",
    "        if item not in my_stopwords:\n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new vectorizer with stemming and transform the documents again\n",
    "\n",
    "# re-create the word list based on the new vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.4.\tCompute the cosine similarity on TF-IDF vectors between the documents with the cosine_similarity function. Which documents are most similar? Can you confirm the judgment of the algorithm by reading the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create a vectorizer that uses TF-IDF weights\n",
    "\n",
    "\n",
    "# calculate the cosine similarity between all documents and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the two most similar documents\n",
    "\n",
    "#TODO: change the indices to the indices of the most similar documents\n",
    "idx1 = 0\n",
    "idx2 = 0\n",
    "\n",
    "print(corpus_4_docs.data[idx1][:500])\n",
    "print('\\n==================\\n')\n",
    "print(corpus_4_docs.data[idx2][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.5.\tExperiment with different similarity metrics as well as with different vector creation methods. Which combination produces the best similarity scores? \n",
    "\n",
    "for different pairwise distances you can use the [pairwise_distances function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# create different vectorizers\n",
    "\n",
    "\n",
    "# calcualte the features\n",
    "\n",
    "\n",
    "# calculate different similarity/distance functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.2.1 Learn a Classifier for the 300-Documents Corpus\n",
    "The 300-documents corpus contains postings from three different news groups. Vectorize\n",
    "the 300-documents corpus and learn a classifier for classifying the postings. Evaluate the\n",
    "classifier using 10-fold X-Validation. Which accuracy does your classifier reach? Increase the\n",
    "performance of your classifier by pruning the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corpus_300_docs = load_files('DataSetEx7/corpus-300docs',encoding='utf-8')\n",
    "\n",
    "class_dist = pd.Series(corpus_300_docs.target).value_counts()\n",
    "plt.bar(class_dist.index, class_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a vectorizer\n",
    "\n",
    "# inspect the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a baseline model with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create a vectorizer for your baseline\n",
    "\n",
    "\n",
    "# define the cross-validation splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# evaluate a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we test different pruning approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define a pipeline and parameter grid\n",
    "\n",
    "\n",
    "# define the cross-validation splits for the nested CV\n",
    "nested_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# define and evaluate a grid search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 7.2.2 Try to do the same classification as in 7.2.1 using word2vec embeddings. You can aggregate word embeddings to get a document representation by applying mean pooling (elementwise average of word vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will download the model (which is 1.3 GB huge) - to change the target folder, execute the following two lines\n",
    "#import os\n",
    "#os.environ[\"GENSIM_DATA_DIR\"] = \"C:/cache\"\n",
    "\n",
    "import gensim.downloader\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return_matrix = []\n",
    "        for doc in X:\n",
    "            mean_vector = np.zeros(self.model.vector_size)\n",
    "            count = 0\n",
    "            for word in self.tokenizer(doc):    \n",
    "                try:\n",
    "                    word_vector = self.model[word]\n",
    "                except KeyError as e:\n",
    "                    continue\n",
    "                count += 1\n",
    "                mean_vector = np.add(mean_vector, word_vector)\n",
    "\n",
    "            return_matrix.append(mean_vector)\n",
    "        return np.array(return_matrix)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# initialize Word2VecVectorizer and run it similarly to 7.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3 Now do the same using BERT embeddings from the huggingface library. Experiment with mean pooling as well as using the [CLS] token representation as document representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class BertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model, tokenizer, use_cls=False):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_cls = use_cls\n",
    "    \n",
    "    def bert_mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        return_matrix = []\n",
    "        \n",
    "        for doc in X:\n",
    "            tokenized = self.tokenizer(doc, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(tokenized['input_ids'])\n",
    "            if self.use_cls:\n",
    "                return_matrix.append(output[1].squeeze(0).numpy())\n",
    "            else:\n",
    "                mean_pooled = self.bert_mean_pooling(output, tokenized['attention_mask'])\n",
    "                return_matrix.append(mean_pooled.squeeze(0).numpy())\n",
    "            \n",
    "        return np.array(return_matrix)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# initialize BertVectorizer and run it similarly to 7.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.3. Learn a Classifier for the Job Postings\n",
    "#### 7.3.1.\tThe Job Postings corpus contains 500 descriptions of open positions belonging to 30 different job categories. The corpus is provided as an Excel file in ILIAS. Vectorize the corpus  and learn a NaÃ¯ve Bayes classifier for classifying the job adds. Evaluate the classifying using 10-fold X-Validation. Analyze the classifier performance and the word list. What do you discover? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "job_postings = pd.read_excel('DataSetEx7/JobPostings.xls')\n",
    "job_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "job_postings_target = job_postings['Category']\n",
    "job_postings_data = job_postings['JobText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot and inspect the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents and show the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.3.2 Experiment with different vector creation and pruning methods as well as different types of classifiers in order to increase the performance. What is highest accuracy that you can reach? Which problem concerning precision and recall does remain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and evaluate a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create a pipeline and parameter grid\n",
    "\n",
    "\n",
    "# create and evaluate a grid search\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
