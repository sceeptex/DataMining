{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Ensembles\n",
    "\n",
    "This exercise is about ensembles. To get familiar with ensembles in scikit-learn, refer to the respective [part in the documentation](https://scikit-learn.org/stable/modules/ensemble.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Warm-up\n",
    "To get you started with ensemble learning, have a look at the `dart` data set that is provided in ILIAS. It contains the positions of darts thrown by four different people. First, we learn two basic classifiers on this data set. In a second step, we apply stacking to improve the performance.\n",
    "\n",
    "- Train a k-NN (with `n_neighbors=1`) and a SVM (with `C=5`) and check their accuracy on the test set\n",
    "- Inspect the classifications on the training data. What can you say about the decision boundaries?\n",
    "- Combine the two classifiers by stacking (use a decision tree as meta learner). Does it improve the accuracy on the test set?\n",
    "- Create a new attribute `distance from centre`. How does the accuracy change?\n",
    "\n",
    "<sub>This task was originally published in https://gormanalysis.com/guide-to-model-stacking-i-e-meta-ensembling/</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train = pd.read_csv('dart/dart_train.csv', sep=',').drop(columns='ID')\n",
    "X_train, y_train = df_train.drop(columns='Competitor'), df_train['Competitor']\n",
    "\n",
    "df_test = pd.read_csv('dart/dart_test.csv', sep=',').drop(columns='ID')\n",
    "X_test, y_test = df_test.drop(columns='Competitor'), df_test['Competitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a k-NN and a SVM and check their performance on the test set.\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the classifications on the training data (using a scatter plot).\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Combine the two classifiers using stacking with a decision tree as meta learner. Measure the accuracy on the test set.\n",
    "# Stacking: scikit-learn has no implementation for Stacking. You can use this -> http://ml-ensemble.com/\n",
    "# HINT: mlens can only work with numerical labels. You can use the LabelEncoder to transform your labels.\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the attribute 'distance_from_centre' and measure the accuracy on the test set (for base classifiers and stacking)\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Mining Cup 2006\n",
    "\n",
    "Now we apply ensembles to the data set of the Data Mining Cup 2006. The task is to predict the attribute `gms_greater_avg` as precisely as possible. We again use accuracy as performance metric.\n",
    "\n",
    "Please make sure to understand the principles of Voting, Bagging, Boosting, and Stacking before you work on this task.\n",
    "\n",
    "- Build a baseline with several classifiers (like k-NN, Decision Tree, SVM, Naive Bayes, Neural Network,..)\n",
    "- Use Bagging to improve the performance of the classifiers. How do they perform compared to a Random Forest?\n",
    "- Try out boosting techniques (like AdaBoost and XGBoost). Do they work better?\n",
    "- Finally, try to get the best results using various kinds of ensembles. You could try to combine several classifiers with Voting or Stacking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('dmc2006/dmc2006_train.txt', sep='\\t', encoding='cp1252').drop(columns=['auct_id', 'gms', 'listing_title', 'listing_subtitle', 'listing_start_date', 'listing_end_date'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='gms_greater_avg'), df['gms_greater_avg'], test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Make baseline predictions for the classifiers k-NN, Decision Tree, SVM, Naive Bayes, Neural Network\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use Bagging to improve the performance. Additionaly, check how a Random Forest classifier performs\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Try out AdaBoost and XGBoost\n",
    "# XGBoost: scikit-learn does not implement XGBoost. Use this -> https://xgboost.readthedocs.io/en/latest/index.html\n",
    "# (If you have problem with running XGBoost, this might help: https://stackoverflow.com/questions/61971851/getting-this-simple-problem-while-importing-xgboost-on-jupyter-notebook)\n",
    "\n",
    "# --- TODO ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to improve your results by applying other ensemble techniques (e.g. Voting, Stacking)\n",
    "# or anything else that comes to your mind. How much can you improve the results?\n",
    "\n",
    "# OPEN END QUESTION - try to improve your results as much as possible!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
